\newif\ifshowsolutions
\showsolutionsfalse
\input{preamble}
\usepackage{listings}
\newcommand{\boldline}[1]{\underline{\textbf{#1}}}

\newcommand{\plotteddata}[3]{
  \begin{figure}[h]
  \caption{#1}
  \centering
  \includegraphics[width=#3\textwidth]{#2}
  \end{figure}
}

\chead{%
  {\vbox{%
      \vspace{2mm}
      \large
      Machine Learning \& Data Mining \hfill
      Caltech CS/CNS/EE 155 \hfill \\[1pt]
      Miniproject 2\hfill
      Released February $17^{th}$, 2017 \\
      {\sc Sonnet Spoofer}\\
      by Shari Kuroyama and Carly Robison
    }
  }
}

\begin{document}
\pagestyle{fancy}

\section{Tokenizing}
\subsection{Methods Used}
What methods did you use and try to tokenize the sonnets?
\begin{itemize}
	\item First we tried the na\"ive tokenization of using each word as a token. With enough states and iterations, this actually learned fairly well.
  \item We trained on individual lines from the poems. When we needed to generate the lines backwards from the rhymes at the end, we trained on individual lines read in backwards.
  \item Words were extracted by splitting on the spaces between them, so hyphenated words were included and every different spelling/conjugation of a word was counted separately.
  \item We kept apostrophes in words, but we removed periods, commas, colons, and parentheses.
\end{itemize}

\subsection{Modifications}
Did you have to make changes to the way you tokenized after running the algorithm and seeing the results? \textbf{Short answer: No.}
\begin{itemize}
	\item We thought about splitting each word into syllables in order to keep to the 10-syllable format of a sonnet. However, we got around that by counting the number of syllables generated per line, and generating new words if the last word was too long.
	\item We also thought about trying to train using bigrams of words as tokens; but again, the na\"ive implementation worked fairly well.
  \item The na\"ive implementation was okay at getting meter, so we didn't see the need to supervise training to improve the meter.
\end{itemize}


\section{Algorithm}
\subsection{Packages Used}
We used the HW5 solutions for unsupervised HMM training. We also used {\tt NLTK}'s dictionary of words to get syllable counts and the {\tt pronouncing} package, which is based on {\tt} NLTK's CMUdict, to find rhymes.
\subsection{Parameters Used}
\begin{itemize}
\item \textbf{Hidden States.} We experimented with different numbers of hidden states. For testing our other algorithms we used 5 hidden states, which produced poems with no sense at all. We found that the poems generated with 10 or 20 states were about the same level of coherence.
\item \textbf{Number of Poems.} With fewer poems, we got more content coherence, but more training data the poems we generated had more grammatical sense.
\item \textbf{Number of Iterations.} We found it hard to tell how many iterations to use; we used anywhere from 15 to 100 for our final poems, and this choice was almost exclusively based on runtime.
\end{itemize}



\section{Poetry Generation}
\subsection{Generation Process}
We iterated through multiple ideas to get a good sonnet. We first took 14 emissions of 10 lines each. We added rhyming by pregenerating pairs of rhyming words that both occurred in Shakespeare's sonnets, and adding them to the end of the lines in the ABABCDCDEFEFGG rhyming scheme. We eventually succeeded in ``seeding'' the HMM emission process with a hidden state chosen according to the emission probabilities for a given word. Our final poetry generation process trained on backwards lines and generated lines backwards from the rhyme at the end. We reversed the line and combined them to make a 14-line sonnet.
\subsection{How to get it to look like a sonnet?}
We used {\tt pronouncing}'s functionality to ensure the correct rhyme scheme. We ensured 10 syllables by predicting sequences of new words when we ran out of space, and doing so until we could finish the line. We generated 14 lines.
\subsection{Do they make sense?}
We did not train or enforce meter, yet many of the sonnets have some kind of iambic meter, but not always carried through all of the lines. It is hard to tell from the most popular words for each state, but some states must have started contained stressed words and others would not have, and alternating between states would give the correct iambic stress pattern. The rhyme and syllable count are correct, because our generation process enforces their correctness. Most of the sonnets make sense if read dramatically, and because of the vocbulary still sound like Shakespeare. Here is the poem we submitted, which was actually generated with 40 states:\\

0\\
\noindent Seasons him make but heart my such being,\\
Rosy gardens all that although is morn,\\
Serving against self beauty i seeing,\\
Their name very so for and proof thy mourn,\\
Of by walls till that touches to the fixed,\\
In lawful who perceived cures whom and page,\\
All to all weed absent better betwixt,\\
Thy please should mayst hold outbraves to age,\\
And you which my us truly in art you,\\
Shall whose shapes but his and with my straying,\\
Which so dearest temptation i woe through,\\
What to that it cross not in be saying,\\
Black enough make muse thee like of thee halt,\\
Of not eyes flies false the viewest did fault.\\

Since all of Shakespeare's sonnets are numbered, we numbered our sonnet 0, because computer scientists zero-index.


\section{Visualization and Interpretation}
\subsection{Hidden State Meaning}
For at least 5 hidden states give a list of the top 10 words that associate with this hidden state and state any common features these groups.

\href{http://setosa.io/markov/index.html#%7B%22tm%22%3A%5B%5B0.007730565105280971%2C0.0018705978701581225%2C0.008997257769494865%2C0.046872665089112114%2C0.017132357678420397%2C0.5049740989055728%2C0.18580214430006262%2C0.03589471816327061%2C0.005051691519477803%2C0.18567390359914554%5D%2C%5B0.0006889869064432156%2C0.001725749573511657%2C0.08303623792799851%2C0.35856907250838377%2C0.006834110959771963%2C0.09883447766378094%2C0.4400048149239121%2C0.0022917262910062994%2C0.003099045249070773%2C0.004915777996119448%5D%2C%5B0.004352310169120523%2C0.00003374857170753811%2C0.07893309217888353%2C0.03504969621230825%2C0.17973403626624573%2C0.05197927221807108%2C0.38617580780919836%2C0.001993957206692008%2C0.0684309975493146%2C0.19331708181845803%5D%2C%5B0.01847339673063565%2C0.00011716811271297575%2C0.0053686704499662456%2C0.007217659326776545%2C0.0131114364122473%2C0.09790807007198406%2C0.3357612960952748%2C0.19231167282727996%2C0.019090541712619404%2C0.31064008826049755%5D%2C%5B0.0005292799668992754%2C0.000009106958256877003%2C0.11938498398125003%2C0.012908707403899735%2C0.025290412997269255%2C0.002543709078006909%2C0.44372230643513905%2C0.1327994603362582%2C0.03644584732634515%2C0.2263661855166703%5D%2C%5B0.11590172132326207%2C0.0001184249227525465%2C0.6167559743387336%2C0.2178261722895216%2C0.0015230893188209324%2C0.0010746553066057945%2C0.031253928231812764%2C0.009623455446019977%2C0.0050448321444109085%2C0.0008777466780638162%5D%2C%5B0.00009810823124581933%2C0.0005857282872976495%2C0.12023939789290301%2C0.0013928507165248656%2C0.16029471282857094%2C0.19986468754454456%2C0.002798821092090132%2C0.001698195070005436%2C0.5125658251853356%2C0.0004616731514790791%5D%2C%5B0.29739410831797636%2C0.00008293375620245867%2C0.03427889081716779%2C0.09238670670530473%2C0.4943090424078646%2C0.005442043398141423%2C0.022242518059520256%2C0.033663505720639544%2C0.0009935967730776145%2C0.019206654044097174%5D%2C%5B0.0013850734881529411%2C0.3661481423422712%2C0.05700586859580324%2C0.060487438002933144%2C0.0023466995970456593%2C0.03176259491611837%2C0.04445745989139778%2C0.29683212140751286%2C0.13168657145314847%2C0.007888030305612264%5D%2C%5B0.007516259934276906%2C0.0038832155176272676%2C0.03727162260773643%2C0.1263838359233777%2C0.18435699513020296%2C0.091835319727203%2C0.046173252910558876%2C0.4451031846685443%2C0.024625654910220927%2C0.032850658670251644%5D%5D%7D}{Visualized transition Matrix, 10 states, 15 iters}
\begin{lstlisting}[mathescape]
state 0    words: ['the', 'but', 'when', 'that', 'my', 'as', 'then', 'which', 'if', 'for']
state 1    words: ['and', 'when', 'for', 'o', 'but', 'in', 'if', 'than', 'to', 'which']
state 2    words: ['the', 'of', 'a', 'my', 'to', 'that', 'on', 'this', 'as', 'thy']
state 3    words: ['that', 'the', 'my', 'to', 'in', 'what', 'are', 'a', 'not', 'as']
state 4    words: ['my', 'that', 'of', 'a', 'me', 'thee', 'do', 'the', 'no', 'have']
state 5    words: ['in', 'so', 'or', 'all', 'not', 'mine', 'like', 'do', 'those', 'will']
state 6    words: ['i', 'thou', 'thy', 'of', 'his', 'be', 'your', 'their', 'thee', 'with']
state 7    words: ['to', 'is', 'with', 'in', 'and', 'doth', 'nor', 'not', 'one', 'then']
state 8    words: ['love', 'fair', 'self', 'new', 'art', 'am', 'me', 'her', 'might', 'by']
state 9    words: ['it', 'love', 'which', 'thee', 'me', 'and', 'you', 'of', 'self', 'can']
\end{lstlisting}

\href{http://setosa.io/markov/index.html#%7B%22tm%22%3A%5B%5B0.020934587501920746%2C0.0012365065372086886%2C0.008966023488046796%2C0.0005254163035177605%2C0.00005739398720642096%2C0.05442859656953214%2C0.02874495344119486%2C0.0027865665990169774%2C0.12522659750072956%2C0.009907355879284638%2C0.01775300836941117%2C0.06830782715185243%2C0.004681828303549693%2C0.0020264386416330303%2C0.01703423651276867%2C0.0109834614006449%2C0.1871577709444298%2C0.024933729905269132%2C0.07554917655659987%2C0.33875852440618615%5D%2C%5B0.0983428562097367%2C0.00595294934849224%2C0.07677443331389919%2C0.03865113088660553%2C0.030178861400254234%2C0.1077150251747156%2C0.01896014388280666%2C0.014632485492390877%2C0.00014963055175719109%2C0.07389528933334574%2C0.024886019433162657%2C0.02648214232219732%2C0.005770201977204964%2C0.02855551754370334%2C0.09472427824915346%2C0.02253433745586691%2C0.06971126389317468%2C0.20569217739416282%2C0.05342701052985624%2C0.0029642456075057453%5D%2C%5B0.010277848599100308%2C0.04322958870515728%2C0.016363286725652244%2C0.0029097063794732364%2C0.041610284896932796%2C0.1690544923288929%2C0.0087113355941646%2C0.015690251407822434%2C0.0951047146771021%2C0.007879564915766598%2C0.05016948538304796%2C0.1620770458855703%2C0.002949166729732471%2C0.07598827054621518%2C0.05339971636826963%2C0.08645487550832597%2C0.008707801158135908%2C0.011684711384476939%2C0.0856452997223557%2C0.052092553083806156%5D%2C%5B0.0006038016229233659%2C0.053794389846176176%2C0.0011293010138962527%2C0.055693934427368495%2C0.12131772207317292%2C0.03539922130071838%2C0.023086228281666357%2C0.17331763390933663%2C0.10848334928951049%2C0.026593236079412632%2C0.000022318751406993486%2C0.1856117580958023%2C0.005537540964735718%2C0.0012441497782212383%2C0.13985913239182357%2C0.021181089397668985%2C0.01312185737257905%2C0.008110332208270357%2C0.0242549593076159%2C0.0016380438876879141%5D%2C%5B0.0010892684139120425%2C0.07036983077421852%2C0.0092649957138241%2C0.006131300906394769%2C0.008237547538171132%2C0.01676261591091805%2C0.21319511534009838%2C0.1590500508529259%2C0.08570846866897316%2C0.04105310455084106%2C0.025836005394405857%2C0.027650156066801906%2C0.00040549905995293614%2C0.08865092839773783%2C0.011621707970995642%2C0.026893830146196424%2C0.06476849555360092%2C0.09108701094610025%2C0.004546857879061316%2C0.04767720991486859%5D%2C%5B0.008673686607200623%2C0.003122401049136201%2C0.07150583702249387%2C0.25230887310949857%2C0.004170955301904398%2C0.03665601675764505%2C0.005942187247769175%2C0.038643296913296446%2C0.2234854588402676%2C0.002293217526504734%2C0.024052077418232887%2C0.0008313012989009762%2C0.001808067428423245%2C0.016666658975674962%2C0.00006541118641869756%2C0.00972477692469836%2C0.005152497683054835%2C0.2779178226685215%2C0.011936077131492986%2C0.005043378908870374%5D%2C%5B0.00009195526168170622%2C0.029017680580306397%2C0.010106590751037689%2C0.000964241512760105%2C0.01605176878198684%2C0.1483701148854986%2C0.012098211212826771%2C0.0042632242087834085%2C0.09868477542715141%2C0.0438932225921881%2C0.03553287284172699%2C0.1858346987391751%2C0.0003761044207499287%2C0.001697825966838172%2C0.044945707484058565%2C0.05480100285788048%2C0.14020057938339142%2C0.09588346798381543%2C0.034256049289640544%2C0.04292990581849798%5D%2C%5B0.0016734373714800558%2C0.05498782813491675%2C0.004294021802465444%2C0.018360222774838186%2C0.026725068208539756%2C0.04178222548099536%2C0.011929463816016285%2C0.04368596154891568%2C0.05764596933404746%2C0.07377699535232925%2C0.04954008777651774%2C0.06509997056577203%2C0.005041061134164556%2C0.05444286024134975%2C0.057422331925265024%2C0.11249158499147685%2C0.17268623062794167%2C0.06602577383082527%2C0.0016625147820004981%2C0.08072639030014397%5D%2C%5B0.16205036806640027%2C0.004053324678180374%2C0.0017573678699267308%2C0.0042567016058535755%2C0.016253736357926877%2C0.08298669214653287%2C0.005098071845980572%2C0.034146497165220766%2C0.09313614594287509%2C0.020381979728698478%2C0.0034894563255382554%2C0.26361825076659834%2C0.0025940094160778375%2C0.03683346787855775%2C0.0037275413170192537%2C0.00044131794481601437%2C0.09929906954466997%2C0.06492761779515804%2C0.08726887649886958%2C0.013679507105098484%5D%2C%5B0.009255425040487708%2C0.029841212824491096%2C0.038640130857146904%2C0.2801411357956158%2C0.15413838196809015%2C0.0043081815061000684%2C0.09469443482850241%2C0.053635937283930867%2C0.07523912346684271%2C0.10106117451617788%2C0.0002147074436426153%2C0.006650810321435792%2C0.005834237585264662%2C0.0015375774066400533%2C0.012409654679936405%2C0.04147591703044886%2C0.06956448878643051%2C0.00044363767850939763%2C0.014174337003008441%2C0.006739493977292458%5D%2C%5B0.020745093337909703%2C0.025559686012442996%2C0.05375728861017865%2C0.04374092093601382%2C0.017357054041739713%2C0.11512541468777311%2C0.06656008033760576%2C0.14336972637422357%2C0.056017356552186176%2C0.022580463675123324%2C0.003717348715907676%2C0.017345918884325275%2C0.001287905054334247%2C0.11927682747087032%2C0.0076783259940099826%2C0.004120813532144197%2C0.1821201860724241%2C0.047459711800334085%2C0.05106920548201223%2C0.001110672428440243%5D%2C%5B0.006723635735144631%2C0.03357000181790578%2C0.04211758489688497%2C0.0009713213583961537%2C0.12015671617330403%2C0.16550268105028462%2C0.08619523858682449%2C0.08692357810534547%2C0.07486166665661916%2C0.08416569029121918%2C0.10075062630951173%2C0.0007854835652720326%2C0.00010489753002833446%2C0.015275841946733796%2C0.003299134076670662%2C0.0007893473888838608%2C0.13689306845372615%2C0.021220207809682902%2C0.018284450069943364%2C0.0014088281776191195%5D%2C%5B0.0057576103486666735%2C0.10684782267738513%2C0.13799716810022808%2C0.051738298183772166%2C0.0999528567089951%2C0.021435142449235826%2C0.06408941209154308%2C0.105669674913834%2C0.041575468006644536%2C0.014413185672410764%2C0.03973201387160304%2C0.02295955276217674%2C0.0006960791058756364%2C0.004942144237267724%2C0.01725744339564085%2C0.1517588910988669%2C0.020800716434304332%2C0.060112600986913046%2C0.007591139837204297%2C0.024672779117431473%5D%2C%5B0.09059478935269964%2C0.007081978449959347%2C0.032502974176325715%2C0.10287455650153868%2C0.007947735458052652%2C0.003041522119148835%2C0.3001516196378236%2C0.07249954857025996%2C0.12263578277687744%2C0.032834871499299395%2C0.030140897890901536%2C0.030736298834685195%2C0.0025403400277918026%2C0.03961792644197906%2C0.0014197529491368056%2C0.026069000959180313%2C0.01734188110877729%2C0.06456198270966251%2C0.013736885016973765%2C0.0016696555189270887%5D%2C%5B0.009918111245330275%2C0.07959699283014043%2C0.022783596185492996%2C0.21334308997277035%2C0.017891364647223535%2C0.04611000401472314%2C0.003149808673588383%2C0.0002907227922421093%2C0.0018205112905214222%2C0.011120734345468808%2C0.00034737843310346636%2C0.026548650449582215%2C0.003672709989210108%2C0.020541746188357465%2C0.028516898311221707%2C0.210622503937524%2C0.06514636465864006%2C0.234012809633734%2C0.0017556533767607821%2C0.002810349024367549%5D%2C%5B0.009724141221214826%2C0.2838390180413807%2C0.010383309038665564%2C0.0034965295309587964%2C0.003966150356039712%2C0.10118584693121595%2C0.013374263347684675%2C0.002154761195036239%2C0.01072804082814806%2C0.0010668901305176464%2C0.019118708787132357%2C0.015903084518837596%2C0.0003239558447338602%2C0.02007703939752782%2C0.0025112787552377903%2C0.035647739215865955%2C0.12444703052040167%2C0.33897459191461077%2C0.0017644879346513489%2C0.0013131324901387723%5D%2C%5B0.3932941840028833%2C0.0018074383170505143%2C0.019867392852425985%2C0.0007656023407675161%2C0.00023903441324233562%2C0.0007196223044765968%2C0.12251419143787835%2C0.0003200866960287889%2C0.0055140126053102876%2C0.015630962977593112%2C0.003701475900182462%2C0.04529966652161936%2C0.00012064477931605569%2C0.002718582018480321%2C0.0026520689703493994%2C0.002089389034127433%2C0.0024083850423691393%2C0.34757429474498736%2C0.02535719213966592%2C0.007405772901244884%5D%2C%5B0.0018741670611363275%2C0.002063471588094867%2C0.0018484227052278808%2C0.0009184888549484246%2C0.03778321562170294%2C0.006044541715683374%2C0.0027462031517908997%2C0.008655163933323823%2C0.13889671579313456%2C0.004445987770314875%2C0.0026003994170928015%2C0.16494554168580147%2C0.002128096920041095%2C0.0008241720027536658%2C0.0013472667537880434%2C0.0024276051643277617%2C0.03361451006397264%2C0.059898973976610134%2C0.515595676007236%2C0.011341379813018523%5D%2C%5B0.3258476671175579%2C0.005387882229318498%2C0.00024386694806909475%2C0.00011164688535776854%2C0.000585608427311506%2C0.004714187243706109%2C0.13530183223512574%2C0.007865182442155427%2C0.05353141662260141%2C0.0008464796441034044%2C0.001690046725623472%2C0.02437005192194954%2C0.0003840985202304517%2C0.19462685616718678%2C0.0001405456044985935%2C0.00046044094770539295%2C0.03432890337843264%2C0.17233741447535256%2C0.004830022175691951%2C0.032395850288020195%5D%2C%5B0.008453225530941988%2C0.03912752770734011%2C0.032713465198939494%2C0.05109974164876684%2C0.08696835063498587%2C0.056765770404219804%2C0.055565301066499324%2C0.06217734995541577%2C0.013956112662808152%2C0.037401851621376006%2C0.1488289543593858%2C0.0023289188138867385%2C0.00556903161028023%2C0.11443432154964865%2C0.005251796223194803%2C0.058026632680845104%2C0.13715228555780457%2C0.003222859225441973%2C0.06929618872075652%2C0.011660314827464331%5D%5D%7D}{Transition Matrix, 20 states, 15 iters}
\begin{lstlisting}[mathescape]
state 0    words: ['of', 'with', 'thee', 'in', 'make', 'heart', 'self', 'show', 'all', 'love']
state 1    words: ['thou', 'and', 'it', 'my', 'time', 'you', 'i', 'is', 'so', 'by']
state 2    words: ['for', 'so', 'but', 'in', 'of', 'thou', 'or', 'i', 'what', 'the']
state 3    words: ['i', 'that', 'no', 'but', 'of', 'they', 'do', 'as', 'thou', 'like']
state 4    words: ['thy', 'i', 'with', 'in', 'than', 'the', 'to', 'that', 'from', 'and']
state 5    words: ['and', 'that', 'is', 'thou', 'shall', 'love', 'so', 'have', 'you', 'then']
state 6    words: ['of', 'me', 'with', 'do', 'the', 'thee', 'for', 'in', 'self', 'is']
state 7    words: ['that', 'i', 'thy', 'am', 'in', 'of', 'what', 'the', 'with', 'it']
state 8    words: ['my', 'of', 'not', 'a', 'to', 'thou', 'i', 'in', 'her', 'be']
state 9    words: ['and', 'that', 'have', 'me', 'which', 'for', 'thee', 'from', 'than', 'as']
state 10   words: ['and', 'that', 'the', 'my', 'for', 'i', 'but', 'with', 'thy', 'as']
state 11   words: ['the', 'i', 'love', 'by', 'thy', 'this', 'your', 'to', 'can', 'in']
state 12   words: ['o', 'and', 'when', 'for', 'if', 'who', 'that', 'then', 'which', 'whilst']
state 13   words: ['and', 'thee', 'to', 'that', 'my', 'heart', 'will', 'be', 'where', 'have']
state 14   words: ['and', 'but', 'when', 'which', 'so', 'nor', 'mine', 'for', 'that', 'why']
state 15   words: ['the', 'i', 'it', 'or', 'which', 'then', 'when', 'thou', 'so', 'by']
state 16   words: ['to', 'his', 'my', 'doth', 'their', 'a', 'and', 'mine', 'thine', 'some']
state 17   words: ['in', 'be', 'thou', 'is', 'not', 'all', 'of', 'love', 'self', 'with']
state 18   words: ['to', 'my', 'thy', 'a', 'in', 'your', 'his', 'and', 'their', 'doth']
state 19   words: ['the', 'and', 'thee', 'from', 'for', 'but', 'can', 'nor', 'this', 'thy']
\end{lstlisting}

\subsection{Properties of Hidden States}
What are some properties of the different hidden states?
e.g. Correlation between hidden states and syllable counts, connotations of words, etc.
[TODO]


\section{Additional Improvements}
\subsection{Rhyme}
We incorporated rhyme by picking seven pairs of rhymes, arranging them in the correct order, and using them at the end of the line. The lines generated forward would often not match with the rhymes at the end, so we switched to generating each line backwards from its ending rhyme.
\subsection{Additional texts: \emph{Hamilton} songs}
Shari?



\section{Conclusion}
\subsection{Division of work}
Shari worked on getting the initial 14-line sonnet, ensuring that lines were 10 syllables with resampling, and incorporating additional source material.

Carly worked on rhyming, seeded and backwards generations of emissions, and interpretation of hidden states.
\subsection{Discoveries}
What are your conclusions/observations about the models you used and the sonnets generated?
\subsection{Challenges}
\subsection{Concluding Remarks}

\begin{itemize}

\item hey there

\end{itemize}

\end{document}
