\newif\ifshowsolutions
\showsolutionsfalse
\input{preamble}

\usepackage{listings}
\usepackage{caption}

\captionsetup{margin=0.5in}
\captionsetup{labelfont=bf}

\newcommand{\boldline}[1]{\underline{\textbf{#1}}}
\newcommand{\plotteddata}[3]{
  \begin{figure}[h]
  \caption{#1}
  \centering
  \includegraphics[width=#3\textwidth]{#2}
  \end{figure}
}

\chead{%
  {\vbox{%
      \vspace{2mm}
      \large
      Machine Learning \& Data Mining \hfill
      Caltech CS/CNS/EE 155 \hfill \\[1pt]
      Miniproject 2\hfill
      Released February $17^{th}$, 2017 \\
      {\sc Sonnet Spoofer}\\
      by Shari Kuroyama and Carly Robison
    }
  }
}



\begin{document}
\pagestyle{fancy}



\section{Tokenizing}
\subsection{Methods Used}
What methods did you use and try to tokenize the sonnets?
\begin{itemize}
	\item First we tried the na\"ive tokenization of using each word as a token. With enough states and iterations, this actually learned fairly well.
  \item We trained on individual lines from the poems. When we needed to generate the lines backwards from the rhymes at the end, we trained on individual lines read in backwards.
  \item Words were extracted by splitting on the spaces between them, so hyphenated words were included and every different spelling/conjugation of a word was counted separately.
  \item We kept apostrophes in words, but we removed periods, commas, colons, and parentheses.
\end{itemize}

\subsection{Modifications}
Did you have to make changes to the way you tokenized after running the algorithm and seeing the results? \textbf{Short answer: No.}
\begin{itemize}
	\item We thought about splitting each word into syllables in order to keep to the 10-syllable format of a sonnet. However, we got around that by counting the number of syllables generated per line, and generating new words if the last word was too long.
	\item We also thought about trying to train using bigrams of words as tokens; but again, the na\"ive implementation worked fairly well.
  \item The na\"ive implementation was okay at getting meter, so we didn't see the need to supervise training to improve the meter.
\end{itemize}


\section{Algorithm}
\subsection{Packages Used}
We used the HW5 solutions for unsupervised HMM training. We used {\tt NLTK}'s dictionary of words to get syllable counts for most words and the {\tt pronouncing} package, which is based on {\tt NLTK}'s {\tt CMUdict}, to find rhymes.  For words that were not in {\tt NLTK}'s {\tt CMUdict}, we used the count_syllables function from a package called Poetry-Tools\footnote{https://github.com/hyperreality/Poetry-Tools/blob/master/poetrytools/countsyl.py}.
\subsection{Parameters Used}
\begin{itemize}
\item \textbf{Hidden States.} We experimented with different numbers of hidden states. For testing our algorithms we started with 5 hidden states, which produced poems with no sense at all. We found that the poems generated with 10 or 20 states were about the same level of coherence.
\item \textbf{Number of Poems.} With fewer poems, we got more content coherence; but with more training data the poems we generated had more grammatical sense.
\item \textbf{Number of Iterations.} We found it hard to tell how many iterations to use; we used anywhere from 15 to 100 for our final poems, and this choice was almost exclusively based on runtime.
\end{itemize}



\section{Poetry Generation}
\subsection{Generation Process}
We iterated through multiple ideas to get a good sonnet. We first took 14 emissions of 10 lines each. We added rhyming by pre-generating pairs of rhyming words that both occurred in Shakespeare's sonnets, and adding them to the end of the lines in the {\tt ABABCDCDEFEFGG} rhyming scheme. We eventually succeeded in ``seeding'' the HMM emission process with a hidden state chosen according to the emission probabilities for a given word. Our final poetry generation process trained on backwards lines and generated lines backwards from the rhyme at the end. We reversed the line and combined them to make a 14-line sonnet.
\subsection{How to get it to look like a sonnet?}
We used {\tt pronouncing}'s functionality to ensure the correct rhyme scheme. We ensured 10 syllables by predicting sequences of new words when we ran out of space, and doing so until we could finish the line. We generated 14 lines.
\subsection{Do they make sense?}
We did not train or enforce meter, yet many of the sonnets have some kind of iambic meter, but not always carried through all of the lines. It is hard to tell from the most popular words for each state, but some states must have contained words which started with stressed syllables and others would not have, and alternating between states would give the correct iambic stress pattern. The rhyme and syllable counts are correct, because our generation process enforces their correctness. Most of the sonnets make sense if read dramatically, and because of the vocabulary they still sound like Shakespeare.  

\begin{figure}[h!]
	\begin{center}
{\bf 0}\\
\noindent Seasons him make but heart my such being,\\
Rosy gardens all that although is morn,\\
Serving against self beauty i seeing,\\
Their name very so for and proof thy mourn,\\
Of by walls till that touches to the fixed,\\
In lawful who perceived cures whom and page,\\
All to all weed absent better betwixt,\\
Thy please should mayst hold outbraves to age,\\
And you which my us truly in art you,\\
Shall whose shapes but his and with my straying,\\
Which so dearest temptation i woe through,\\
What to that it cross not in be saying,\\
Black enough make muse thee like of thee halt,\\
Of not eyes flies false the viewest did fault.
	\end{center}
	\caption{
		Submitted poem, generated with 40 states.  Since Shakespeare's sonnets are numbered 1 to 154, and because computer scientists zero-index, we numbered our sonnet 0.  
	}
\end{figure}

\section{Visualization and Interpretation}
\subsection{Hidden State Meaning}

\begin{lstlisting}[mathescape]
######################################################################################################
            Most likely words for each state. 10 states 20 iters
######################################################################################################
state 0    words: ['for', 'and', 'with', 'that', 'when', 'to', 'which', 'by', 'where',
'how', 'then', 'let', 'o', 'within', 'so', 'all', 'why', 'like', 'in', 'as']
state 1    words: ['of', 'love', 'self', 'in', 'heart', 'eye', 'eyes', 'have', 'to',
'from', 'own', 'on', 'am', 'beauty', 'not', 'no', 'world', 'sweet', 'will', 'doth']
state 2    words: ['the', 'thy', 'my', 'with', 'to', 'it', 'or', 'they', 'mine', 'his',
'i', 'and', 'that', 'thou', 'your', 'a', 'have', 'some', 'dost', 'when']
state 3    words: ['to', 'of', 'a', 'thou', 'me', 'in', 'that', 'than', 'what', 'her', 
'all', 'such', 'will', 'thy', 'as', 'can', 'but', 'this', 'doth', 'thee']
state 4    words: ['in', 'all', 'not', 'to', 'that', 'this', 'on', 'i', 'so', 'from', 'he', 
'are', 'what', 'like', 'as', 'is', 'art', 'which', 'me', 'for']
state 5    words: ['and', 'but', 'so', 'as', 'that', 'nor', 'then', 'by', 'thou', 'which', 
'or', 'how', 'when', 'thus', 'for', 'if', 'o', 'seem', 'not', 'yet']
state 6    words: ['is', 'be', 'of', 'that', 'thee', 'more', 'do', 'love', 'now', 'and', 
'praise', 'hath', 'their', 'have', 'are', 'still', 'doth', 'being', 'with', 'live']
state 7    words: ['i', 'not', 'thee', 'me', 'that', 'time', 'o', 'she', 'be', 'you', 'art', 'him', 'it', 'truth', 'self', 'love', 'is', 'he', 'day', "time's"]
state 8    words: ['thou', 'and', 'if', 'yet', 'but', 'to', 'for', 'who', 'when', 'that', 'those', 'no', 'so', 'as', 'with', 'which', 'can', 'do', 'therefore', 'by']
state 9    words: ['my', 'the', 'i', 'thy', 'to', 'a', 'your', 'his', 'thee', 'you', 'this', 'it', 'mine', 'thine', 'should', 'me', 'of', 'their', 'sweet', 'one']
\end{lstlisting}

Thanks to the team Hidden Statespeare, we found this tool for visualizing our transition matrices.
\href{http://setosa.io/markov/index.html#%7B%22tm%22%3A%5B%5B0.14911780701621213%2C0.00010572295755836036%2C0.016198406555885045%2C0.0016568860217401576%2C0.04232650579942076%2C0.000560883741495299%2C0.00023999264017775384%2C0.1203754239165369%2C0.0026129355973558504%2C0.6668054357536208%5D%2C%5B0.000437708826413381%2C0.028375420362495806%2C0.0023637788632358127%2C0.13011855858214588%2C0.018582583431150906%2C0.0022389705666316685%2C0.5318743969327084%2C0.032805978633223716%2C0.0000010931256096617502%2C0.2532015106763805%5D%2C%5B0.00006041324386681654%2C0.5654530204127515%2C0.0025598667500555416%2C0.003034649977868434%2C0.021699996415127824%2C0.0000027974488129123203%2C0.003539809124666394%2C0.40339013133636925%2C0.000014370033986913576%2C0.0002449452564950452%5D%2C%5B0.0018612760388129707%2C0.002037433477895126%2C0.5052102015485214%2C0.0012349449664440482%2C0.0028678277403334836%2C0.000008450824570543032%2C0.3669248947703795%2C0.09427276112942308%2C0.00006631798695654071%2C0.02551589151666174%5D%2C%5B0.000005296614577609269%2C0.0039100305364940125%2C0.022683952820654094%2C0.3289977811068986%2C0.06501344300024292%2C0.000016616851749767245%2C0.0068482321246508574%2C0.06255119450146387%2C0.00007901197443834435%2C0.5098944404688287%5D%2C%5B0.06596993404458429%2C0.0028920358275504533%2C0.04987788005753903%2C0.00011843324358164615%2C0.2533237364632281%2C0.017261484443527722%2C0.005753403952515774%2C0.012414018050341188%2C0.5893715650158543%2C0.003017508901265842%5D%2C%5B0.00038578516655109207%2C0.0006775888632618098%2C0.026116244137306847%2C0.5252840113309079%2C0.31873519580796067%2C0.011736130070108557%2C0.00007430669906052717%2C0.019957117544793346%2C0.0007742733241273738%2C0.09625934705591993%5D%2C%5B0.3956861993382268%2C0.00003880756861684995%2C0.0008000081361946689%2C0.003816403036142031%2C0.0008442854864070178%2C0.5496426596086107%2C0.01738939300221058%2C0.0008627634984346483%2C0.029493962881793668%2C0.0014255174433633496%5D%2C%5B0.002212186572691488%2C0.0008943320348949857%2C0.3183029705397009%2C0.0008140179500970441%2C0.35557447192893055%2C0.003038969213980188%2C0.019511172115105467%2C0.24091563755598489%2C0.05035203572619779%2C0.008384206362414054%5D%2C%5B0.000017504689502350425%2C0.9194083664937497%2C0.00000270196418012294%2C0.002259737911401638%2C0.0020669584746729297%2C2.8227225867623864e-7%2C0.05954567015206327%2C0.011839395679871969%2C3.4925508750036434e-7%2C0.004859033107211286%5D%5D%7D}{Visualized Transition Matrix, 10 states, 20 iters}

\plotteddata{Transition Matrix, 10 states 20 iters}{visualize}{.8}


\subsection{Properties of Hidden States}
From looking at the most common words for each state, we can extract some part of speech meaning:

\begin{compactitem}
\item state 0/A: conjunctions \& prepositions, simile making words, who how which where when why
\item state 1/B: parts of the body
\item state 2/C: possessive pronouns \& adjectives
\item state 3/D: object/subject pronouns, comparisons
\item state 4/E: adjectives
\item state 5/F: conjunctions, time comparisons
\item state 6/G: continuousness (being, still) and possession
\item state 7/H: concepts (art, truth, self, love) and time
\item state 8/I: more comparison words
\item state 9/J: possessive adjectives and pronouns
\end{compactitem}

We also examined the top 20 mutisyllabic words for each state. From these we got the following additional characterization of the states:
\begin{compactitem}
\item state 0/A: negative connotation (against, thievish, wasteful), contrasting (although, without, only)
\item state 1/B: nouns, positive (beauty, treasure, glory), spiritual (mistress', spirit, body), time (minutes, summer)
\item state2/C: plural words (beauty's, woman's, roses, hours), possessive (our, others'), emotional (happy, fire, truly, wretched, desire)
\item state 3/D: prepositions (upon, o'er, wherein), spiritual (buried, holy), values (honour, reason, respect)
\item state 4/E: prepositions (upon, before, again, above), spiritual (heaven, nature, believe)
\item state 5/F: nouns/nature's trials (water, winter, elements) somewhat sad adjectives (tired, absent, vainly, oppressed, alas)
\item state 6/G: loss (alone, decay, away, father, morrow, heaven)
\item state 7/H: beautiful adjectives (beauty, beauteous, lovely, flowers, fairest, heavenly), physical attributes (wrinkles, fingers)
\item state 8/I: sounds (music), direction of feeling (against, attending, mutual, towards), bad thoughts (pity, despised) and good thoughts (fairer, praises, happy)
\item state 9/J: possessive adjectives (summer's, beauty's, heaven's, precious, beloved), obscured from view (seeming, painting, inward)
\end{compactitem}




What are some properties of the different hidden states?
e.g. Correlation between hidden states and syllable counts, connotations of words, etc.
[TODO]


\section{Additional Improvements}
\subsection{Rhyme}
We incorporated rhyme by randomly picking seven pairs of rhymes, arranging them in the correct order, and using them at the end of the line. The lines generated forward would often not make sense with forcing the rhymes at the end, so we switched to generating each line backwards from its ending rhyme.

\subsection{Additional texts: \emph{Hamilton} songs}
We also trained our sonnets on the first three songs of \emph{Hamilton}.  These songs had a much larger variety of punctuation, in both symbols used and placement (for example, many lines ended with a dash); so in addition to removing periods, commas, colons, and parentheses, we also removed exclamation points, question marks, double and single quotes, em dashes, and ellipses.  This resulted in some fairly strange grammar, particularly when it came to removing single quotes in contractions (producing words like ``n'' and ``evry'', but also unclear grammar in words like ``its''/``it's''); but we still chose to remove single quotes because of the number of single-quoted quotations.  

The songs in Hamilton also had many lines which were \emph{much} shorter than ten syllables, which may have negatively contributed to the sense of the resulting poem.

\begin{figure}[h!]
	\begin{center}
{\bf 1776}\\
Throwing america some up not what, \\
Afar take a it what do you with split, \\
Not were im but im i dead gonna but,  \\
Gonna new name face shes good out of it, \\
Two yo sugar to they a guarantee,  \\
To run i its not but his to am gods,  \\
Cousin are he ready the he squat oui,  \\
Came and time when anybody down odds, \\
See socially and burr the you forgot,  \\
Every york a get or to plenty,  \\
Yo im dying but not so so its got,  \\
A whoa a n up yo and i many, \\
Im hey but our aint until was do,  \\
Dying down the is to hey x i brew. 
	\end{center}
	\caption{
		The above sonnet, which we submitted on Piazza along with our Shakespearean sonnet, was trained using 20 states over 100 iterations.  It makes much less sense than the Shakespearean one.
	}
\end{figure}



\section{Conclusion}
\subsection{Division of work}
Shari worked on preprocessing, getting the initial 14-line sonnet, ensuring that lines were 10 syllables with resampling, and incorporating additional source material.

Carly worked on rhyming, seeded and backwards generations of emissions, and interpretation of hidden states.

\subsection{Discoveries}
What are your conclusions/observations about the models you used and the sonnets generated?
\begin{itemize}
	\item We conclude that this model captures the complexity of Shakespearean sonnets fairly well.  With enough training data, we produced poems which seemed quite similar to Shakespeare.  We were quite happy with the quality of our generated poems.  
	\item On the other hand, a sonnet is long enough that our generated poems didn't stay on one topic from start to finish.  This could be because our HMM only takes into account the likelihood of transitioning between two states, without looking at any prior information, and could definitely be improved upon by using more than one layer of hidden states.
\end{itemize}

\subsection{Challenges}
\begin{itemize}
	\item Trying to get the poems to make sense was very difficult.  For the initial sonnets, there was a lot of very poor grammar.  We mostly fixed this by running the training algorithm on more data, with more states, and more iterations.
Relatedly, incorporating different source material was harder than expected, mostly because the structure of \emph{Hamilton} songs is so different from that of Shakespeare's sonnets.  We had to make a lot of edits to the preprocessing code to get it to read the songs, and still ended up with poems which made less sense than the Shakespearean ones.
	\item We also surprisingly found ourselves making a good number of edits to the {\tt HMM.py} code from Homework 5, to get the functions to output in different formats, and to generated seeded emissions.  For seeding emissions, given a word we had to figure out which state that word had come from in order to produce some most likely next words.  In order to do that, we chose to use the probability distribution of state transitions, but could have used the most likely state, or even kept track of the actual state throughout.
	\item Finally, when trying to visualize and interpret the hidden states, we found a lot of the most-used words in each state had a huge overlap from state to state.  To solve this, we tried separating out the top unique words in each state.  However, we still had trouble separating the states into parts of speech, as many states seemed to be separated by topic (e.g.\ loneliness, or flowery language) than by function.
\end{itemize}



\subsection{Concluding Remarks}

This project was a lot of fun.  We really enjoyed playing around with the different poems, and reading them aloud to everyone who passed by.  







\end{document}
